/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
config loaded.
nll weight: 1.0, pixel weight: 1.0, vgg weight: 1.0
use crop dataset for vgg loss
train dataset: sr-implicit-downsampled-fast-crop
val dataset: sr-implicit-downsampled-fast
train dataset: size=16000
  inp: shape=(3, 48, 48)
  coord: shape=(48, 48, 2)
  cell: shape=(2,)
  gt: shape=(3, 48, 48)
/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
val dataset: size=1600
  inp: shape=(3, 48, 48)
  coord: shape=(48, 48, 2)
  cell: shape=(2,)
  gt: shape=(3, 48, 48)
device: cuda:0
model: #params=1.9M
lr: 0.0001
train:   0%|                                                               | 0/1000 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()











































































































train: 100%|████████████████████████████████████████████████████▊| 997/1000 [03:36<00:00,  4.67it/s]
val:   0%|                                                                  | 0/100 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()


val:  67%|██████████████████████████████████████▏                  | 67/100 [00:03<00:01, 26.14it/s]
train:   0%|                                                               | 0/1000 [00:00<?, ?it/s]















































































































train:   0%|                                                               | 0/1000 [00:00<?, ?it/s]















































































































val: 100%|████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 26.57it/s]
















































































































train:   0%|                                                       | 2/1000 [00:01<11:51,  1.40it/s]















































































































train:   0%|                                                               | 0/1000 [00:00<?, ?it/s]















































































































train:   0%|                                                               | 0/1000 [00:00<?, ?it/s]















































































































train:   0%|                                                               | 0/1000 [00:00<?, ?it/s]














































































































val:  73%|█████████████████████████████████████████▌               | 73/100 [00:04<00:01, 26.11it/s]















































































































val:  73%|█████████████████████████████████████████▌               | 73/100 [00:04<00:01, 26.14it/s]
















































































































val: 100%|████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 26.38it/s]















































































































train:   0%|                                                               | 0/1000 [00:00<?, ?it/s]















































































































val:  97%|███████████████████████████████████████████████████████▎ | 97/100 [00:05<00:00, 26.26it/s]















































































































val:  85%|████████████████████████████████████████████████▍        | 85/100 [00:04<00:00, 26.15it/s]















































































































val:  82%|██████████████████████████████████████████████▋          | 82/100 [00:04<00:00, 26.31it/s]













































































































train:  99%|████████████████████████████████████████████████████▋| 993/1000 [03:35<00:01,  4.64it/s]


train:   0%|                                                               | 0/1000 [00:00<?, ?it/s]













































































Traceback (most recent call last):
  File "/content/drive/MyDrive/LINF/train.py", line 418, in <module>
    main(config, save_path)
  File "/content/drive/MyDrive/LINF/train.py", line 316, in main
    train_loss = train(train_loader, model, optimizer, epoch, config['patch'])
  File "/content/drive/MyDrive/LINF/train.py", line 164, in train
    loss.backward()
  File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt